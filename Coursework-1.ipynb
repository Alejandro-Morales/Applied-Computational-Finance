{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <font color = black size=7>Applied C.F Assignment 1</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <font color = black size=5>Introduction</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this report is to solve the system of equations **Ax = b** with Python using different methods. We will apply two different **LU** decomposition techniques: Doolittle's Method and Crout's Method to then solve the system of equations using forward and backward substitution. Also we will apply Gauss-Seidel and SOR iterative techniques and experiment with different error values to observe how fast each of them converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strictly Diagonally Dominant Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **strictly diagonally dominant** matrix **M** is a square matrix that for each row, the absolute value of the term that lies in the leading diagonal is greater than the sum of the absolute values of the remaining terms of that row.\n",
    "\\begin{gather}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{bmatrix}\n",
    "\\end{gather}\n",
    "\n",
    "$\\mid a \\mid  >  \\mid b \\mid + \\mid c \\mid$ and $\\mid e \\mid  >  \\mid d \\mid + \\mid f \\mid$ and $\\mid  i \\mid  >  \\mid g \\mid + \\mid h \\mid $\n",
    "\n",
    "\n",
    "When a matrix is **strictly diagonally dominant** it means that it can be inverted. This guarantees convergence when using iterative techniques to solve the system of equations in matrix **M**. <p> We built a function that returns True if the matrix M is strictly diagonally dominant, False otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonallyDom(M):\n",
    "    m = len(M)\n",
    "    for i in range(m):\n",
    "        D = np.absolute(M[i][i])\n",
    "        if D == 0:\n",
    "            return False\n",
    "        suma = 0\n",
    "        for j in range(m):\n",
    "            if i != j:\n",
    "                suma = suma + np.absolute(M[i][j])\n",
    "        if D <= suma:\n",
    "            return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a function that tries to convert a matrix M to a strictly diagonally dominant matrix. The function takes the index of the largest absolute value in each row, and reorders that matrix by putting that row in the index taken. This function only modifies rows, it will return that reordered matrix and the reordered vector b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryDiagonally(M,b):\n",
    "    if diagonallyDom(M):\n",
    "        return M, b\n",
    "    \n",
    "    s = len(M)\n",
    "    A = np.zeros((s,s))\n",
    "    y = np.zeros(s)\n",
    "    \n",
    "    maximum = 0\n",
    "    index = 0\n",
    "    for i in range(s):\n",
    "        maximum = 0\n",
    "        index = 0\n",
    "        for j in range(s):\n",
    "            if np.absolute(M[i][j]) > maximum:\n",
    "                maximum = np.absolute(M[i][j])\n",
    "                index = j\n",
    "        A[index] = M[i]\n",
    "        y[index] = b[i]\n",
    "    \n",
    "    return A,y\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangular Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates if **M** is a triangular matrix. It returns a 1 if it is a lower triangular matrix, a 2 if it is an upper triangular matrix and 0 otherwise. Example of an upper triangular matrix:\n",
    "\n",
    "\\begin{gather}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "3 & 1 & 7\\\\\n",
    "0 & 5 & 3\\\\\n",
    "0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular(M):\n",
    "    n = len(M)\n",
    "    if n != len(M[0]):\n",
    "        return 0\n",
    "    upper = True\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if np.round(M[i][j], 3) !=0:\n",
    "                upper = False\n",
    "    lower = True\n",
    "    for i in range(n):\n",
    "        a = n-i-1\n",
    "        for j in range(a):\n",
    "            b = n-j-1\n",
    "            if np.round(M[i][b], 3) !=0:\n",
    "                lower = False\n",
    "    if lower:\n",
    "        return 1\n",
    "    elif upper:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a square **nxn** matrix **A** as a product of two matrices **L** and **U**, **A = LU**. Where **L** is a lower triangular matrix and **U** and upper triangular matrix. By doing this factorization, the problem of solving the system of equations **Ax = b** becomes much easier, we can solve it by applying a forward substitution and a backward substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doolittle's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doolittle's Method** is a type of **LU** decomposition. Gaussian Elimination is applied to **A** in order to get **U**. In this method, **L** is a unit lower triangular matrix, meaning that every term in its leading diagonal is a 1. The entries that go below the leading diagonal of the **L** matrix are the multipliers in the Gaussian Elimination process. \n",
    "\n",
    "The following function decomposes the matrix **A** into **L** and **U** using **Doolittle's Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doolittle(M):\n",
    "    M = M*1.0\n",
    "    \n",
    "    s = len(M)\n",
    "    \n",
    "    I = np.identity(len(M))\n",
    "\n",
    "    for h in range(s-1):\n",
    "        for i in range(h,s-1):\n",
    "            if M[i+1][h] != 0:\n",
    "                num = float(M[i+1][h]/M[h][h])\n",
    "                I[i+1][h] = num\n",
    "                for j in range(s):\n",
    "                    temp = M[i+1][j]\n",
    "                    M[i+1][j] = temp - (num*M[h][j])\n",
    "    return I,M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crout's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crout's Method** is another type of **LU** decomposition. In this method, the upper triangular matrix **U** has the unit leading diagonal, differing from **Doolitle's Method** where the unit leading diagonal is in the lower triangular matrix **L**.\n",
    "\n",
    "The following function decomposes the matrix **A** into **L** and **U** using **Crout's Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crout(M):\n",
    "    a = len(M)\n",
    "    L = np.zeros([a,a])\n",
    "    U = np.identity(a)\n",
    "\n",
    "    for i in range(a):\n",
    "        for j in range(i+1):\n",
    "            for h in range(j+1):\n",
    "                if h == 0:\n",
    "                    L[i][j] = M[i][j]\n",
    "                else:\n",
    "                    L[i][j] = L[i][j] -U[h-1][j]*L[i][h-1]\n",
    "\n",
    "        for m in range(a-1-i):\n",
    "            for n in range(i+1):\n",
    "                if n == 0:\n",
    "                    U[i][m+1+i] = M[i][m+1+i]/L[i][i]\n",
    "                else:\n",
    "                    U[i][m+1+i] = U[i][m+1+i] -(U[n-1][m+1+i]*L[i][n-1]/L[i][i])\n",
    "    return L,U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ax = b using LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, when we decompose **A** into **L** and **U** it is very easy to solve the system **Ax = b** using forward and backward substitutions.\n",
    "\n",
    "We have that **Ax =(LU)x = L(Ux) = b**. We can devide this into two smaller problems.\n",
    "\n",
    "**1. Lz = b**\n",
    "\n",
    "**2. Ux = z**.\n",
    "\n",
    "First we solve for **z** in **1** using **forward substitution** and then, using **z**, we solve for **x** in **2** **using backward** substitution. The following **solver** function uses the algorithm mentioned above to solve the system of equations. It takes as inputs **L**,**U** and **b** and returns **x**. It also verifies that the inputs **L** and **U** are lower and upper triangular matrices respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solver(L,U,b):\n",
    "    # Forward substitution\n",
    "    if triangular(L) != 1:\n",
    "        return \"L is not a lower triangular matrix\"\n",
    "    n = len(b)\n",
    "    z = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        z[i] = b[i]\n",
    "        for j in range(i):\n",
    "            z[i] = z[i]-L[i][j]*z[j]    \n",
    "        z[i] = z[i]/L[i][i]\n",
    "    print(\"z = \", z)\n",
    "    \n",
    "    if triangular(U) != 2:\n",
    "        return \"U is not an upper triangular matrix\"\n",
    "    \n",
    "    # Backward substitution\n",
    "    y = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        a = n-1-i\n",
    "        y[a] = z[a]\n",
    "        for j in range(i):\n",
    "            b = n-1-j\n",
    "            y[a] = y[a]-U[a][b]*y[b]    \n",
    "        y[a] = y[a]/U[a][a]\n",
    "    print(\"x = \",y)\n",
    "    return y\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss-Seidel Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Gauss Seidel Method** is an iterative method that is used solve linear systems of equations. Convergence is guaranteed when the matrix is strictly diagonally dominant. The following function applies this method to solve the system of equations **Ax = b**. It takes as inputs a matrix, a vector **b** and an initial guess **x0**, returning the resulting **x** after the final iteration, and the number of iterations. This function iterates until it reaches consistency to the tolerance $\\varepsilon$ using  the **infinity norm** as convergence criteria. The function also takes a boolean pr parameter that is used only to indicate if we want to print the results.\n",
    "\n",
    "**Infinity Norm**\n",
    "\n",
    "$l_\\infty = \\frac{\\mid \\mid \\vec{x}^{(k+1)}-\\vec{x}^{(k)}\\mid \\mid_\\infty}{\\mid \\mid \\vec{x}^{(k)}\\mid \\mid_\\infty} < \\varepsilon$\\\\\n",
    "\n",
    "$\\varepsilon$ is the tolerance > 0.\n",
    "\n",
    "$\\mid \\mid \\vec{x} \\mid \\mid_\\infty = max \\mid x_i \\mid $ \n",
    "\n",
    "$1\\leq i \\leq n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GS(M,x,p,pr):\n",
    "    prev = np.zeros((2,len(M)),float)\n",
    "\n",
    "    m = len(M)\n",
    "\n",
    "    inf_norm = 1\n",
    "    count = 0\n",
    "    while inf_norm > p :\n",
    "        for i in range(m):\n",
    "            prov = b[i]\n",
    "            for j in range(m):\n",
    "                if i != j:\n",
    "                    prov = prov -M[i][j]*x[j]\n",
    "            x[i] = prov/M[i][i]\n",
    "\n",
    "        prev[1] = prev[0]\n",
    "        prev[0] = x\n",
    "\n",
    "        if count != 0:\n",
    "            inf_norm = (max(np.absolute(prev[0]-prev[1])))/max(np.absolute(prev[1]))\n",
    "            if pr:\n",
    "                print(\"error = \",inf_norm)\n",
    "        count = count +1\n",
    "        if pr:\n",
    "            print(x)\n",
    "            print(\"-----------------\")\n",
    "    return x, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successive-Over Relaxation (SOR) Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a technique that can be applied to iterative methods such as the **Gauss-Seidel Method**. The aim of the **SOR** method is to introduce an acceleration factor **w** to speed up convergence where **1** **<** **w** **<** **2**.\n",
    "\n",
    "The following function applies **SOR** to the **Gauss-Seidel Method** to solve the system of equations **Ax = b**. It takes as inputs a matrix, a vector **b**,initial guess **x0**, and the acceleration factor **w** returning the resulting **x** after the final iteration. This function iterates until it reaches consistency to the tolerance $\\varepsilon$ using  the **infinity norm** as convergence criteria. Again, the function also takes a boolean pr parameter that is used only to indicate if we want to print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOR(M,x,w,p,pr):\n",
    "    \n",
    "    prev = np.zeros((2,len(M)),float)\n",
    "\n",
    "    m = len(M)\n",
    "\n",
    "    inf_norm = 1\n",
    "    count = 0\n",
    "    while inf_norm > p:\n",
    "        for i in range(m):\n",
    "            prov = b[i]\n",
    "            for j in range(m):\n",
    "                if i != j:\n",
    "                    prov = prov -M[i][j]*x[j]\n",
    "            pre = x[i]\n",
    "            x[i] = ((1-w)*pre)+(w*prov/M[i][i])\n",
    "\n",
    "        prev[1] = prev[0]\n",
    "        prev[0] = x\n",
    "\n",
    "        if count != 0:\n",
    "            inf_norm = (max(np.absolute(prev[0]-prev[1])))/max(np.absolute(prev[1]))\n",
    "            if pr:\n",
    "                print(\"error = \",inf_norm)\n",
    "        count = count +1\n",
    "        if pr:\n",
    "            print(x)\n",
    "            print(\"-----------------\")\n",
    "    return x, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing the system of equations with the matrix **A** and the vector **b**\n",
    "We will solve **Ax = b** using the methods mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,3,-1,8], \n",
    "              [-1,11,-1,3],\n",
    "              [2,-1,10,-1],\n",
    "              [10,-1,2,0]])\n",
    "\n",
    "b = np.array([15,25,-11,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strictly Diagonally Dominant Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test if our Matrix **A** is strictly diagonally dominant using the function **diagonallyDom(M)** we defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagonallyDom(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a **False** result,  this means that our matrix **A** is not strictly diagonally dominant, we will use our previously defined **tryDiagonally()** function to rearrange the rows in matrix **A** and see if we can convert it into a strictly diagonally dominant matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "[[10. -1.  2.  0.]\n",
      " [-1. 11. -1.  3.]\n",
      " [ 2. -1. 10. -1.]\n",
      " [ 0.  3. -1.  8.]]\n",
      "b = \n",
      "[  6.  25. -11.  15.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, b = tryDiagonally(A,b)\n",
    "print(\"A = \")\n",
    "print(A)\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "diagonallyDom(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that matrix **A** and vector **b** have been rearranged and tested for with our **diagonallyDom()**. We can see that it returns **True**, this means that matrix **A** became indeed a strictly diagonally dominant matrix. We will work with this new modified **A** matrix and **b** vector from now on. Once we transformed our matrix **A** into a strictly diagonally dominant matrix, we proceeded to decompose it into **L** and **U** using **Doolittle's** and **Crout's** method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doolittle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = \n",
      "[[ 1.     0.     0.     0.   ]\n",
      " [-0.1    1.     0.     0.   ]\n",
      " [ 0.2   -0.073  1.     0.   ]\n",
      " [ 0.     0.275 -0.082  1.   ]]\n",
      "U = \n",
      "[[10.    -1.     2.     0.   ]\n",
      " [ 0.    10.9   -0.8    3.   ]\n",
      " [ 0.     0.     9.541 -0.78 ]\n",
      " [ 0.    -0.     0.     7.111]]\n"
     ]
    }
   ],
   "source": [
    "L,U = Doolittle(A)\n",
    "print(\"L = \")\n",
    "print(np.round(L, 3))\n",
    "\n",
    "print(\"U = \")\n",
    "print(np.round(U, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Doolittle's** method we can see that our resulting **L** and **U** matrices are indeed lower and upper triangular matrices respectively, with **L** containing a unit leading diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., -1.,  2.,  0.],\n",
       "       [-1., 11., -1.,  3.],\n",
       "       [ 2., -1., 10., -1.],\n",
       "       [ 0.,  3., -1.,  8.]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.dot(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verified that our resulting matrices **L** and **U** were correct by doing a matrix multiplication between them and verifying that the result was **A**, we can see above that **LU = A** .\n",
    "\n",
    "Then we use our **solver** function to calculate the intermediate vector **z** in [**Ax =(LU)x = L(Ux) = b**] and the final solution **x** using forward and backward substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  [  6.          25.6        -10.32110092   7.11057692]\n",
      "x =  [ 1.  2. -1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2., -1.,  1.])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver(L,U,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "solution = np.linalg.inv(A).dot(b)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that using our solver function we got a result of **x = [1,2,-1,1]**, we verified that this solution is correct using **x = inv(A)b**, we can see that it returns the same result.\n",
    "\n",
    "Now we move forward to do the same process with **Crout's method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.     0.     0.     0.   ]\n",
      " [-1.    10.9    0.     0.   ]\n",
      " [ 2.    -0.8    9.541  0.   ]\n",
      " [ 0.     3.    -0.78   7.111]]\n",
      "[[ 1.    -0.1    0.2    0.   ]\n",
      " [ 0.     1.    -0.073  0.275]\n",
      " [ 0.     0.     1.    -0.082]\n",
      " [ 0.     0.     0.     1.   ]]\n"
     ]
    }
   ],
   "source": [
    "L,U = Crout(A)\n",
    "print(np.round(L, 3))\n",
    "print(np.round(U, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  [ 0.6         2.34862385 -1.08173077  1.        ]\n",
      "x =  [ 1.  2. -1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2., -1.,  1.])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver(L,U,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in this method, we get the unit leading diagonal in the **U** matrix instead of in the **L** matrix as in Doolittle's method,but when we apply our solver function using forward and backward substitution we still get the correct result of **x = [1,2,-1,1]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Seidel Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solved the system of equations **Ax = b** using the GS function explained above that computes the Gauss-Seidel Method. Since we verified that our matrix **A** is strictly diagonally dominant, we know this method will converge to the actual solution. We will iterate until the method reaches consistency to the tolerance $\\varepsilon$ using  the **infinity norm** as convergence criteria.\n",
    "\n",
    "In this first part we begin with an initial guess of **x0 = [0,0,0,0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6         2.32727273 -0.98727273  0.87886364]\n",
      "-----------------\n",
      "error =  0.18484375000000003\n",
      "[ 1.03018182  2.03693802 -1.0144562   0.98434122]\n",
      "-----------------\n",
      "error =  0.016388814658793216\n",
      "[ 1.00658504  2.00355502 -1.00252738  0.99835095]\n",
      "-----------------\n",
      "error =  0.002856953090344185\n",
      "[ 1.00086098  2.00029825 -1.00030728  0.99984975]\n",
      "-----------------\n",
      "error =  0.0003847917873479008\n",
      "[ 1.00009128  2.00002134 -1.00003115  0.9999881 ]\n",
      "-----------------\n",
      "error =  4.1457869928006095e-05\n",
      "[ 1.00000836  2.00000117 -1.00000275  0.99999922]\n",
      "-----------------\n",
      "Number of iterations =  6\n"
     ]
    }
   ],
   "source": [
    "e = 0.0001\n",
    "x0 = np.zeros(len(A),float)\n",
    "pr = True\n",
    "x, it = GS(A,x0,e,pr)\n",
    "print(\"Number of iterations = \", it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use an initial guess of **x0 = [1,1,1,1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5         2.13636364 -0.88636364  0.96306818]\n",
      "-----------------\n",
      "error =  0.22978723404255322\n",
      "[ 0.99090909  2.01957645 -0.99991736  0.99266916]\n",
      "-----------------\n",
      "error =  0.008612275598771455\n",
      "[ 1.00194112  2.0021833  -1.00090298  0.99906839]\n",
      "-----------------\n",
      "error =  0.0009864457246363382\n",
      "[ 1.00039893  2.00020825 -1.00015212  0.99990289]\n",
      "-----------------\n",
      "error =  0.0001738197994263564\n",
      "[ 1.00005125  2.00001731 -1.00001823  0.99999123]\n",
      "-----------------\n",
      "error =  2.293582118326794e-05\n",
      "[ 1.00000538  2.00000122 -1.00000183  0.99999931]\n",
      "-----------------\n",
      "Number of iterations =  6\n"
     ]
    }
   ],
   "source": [
    "e = 0.0001\n",
    "x0 = np.ones(len(A),float)\n",
    "pr = True\n",
    "x, it = GS(A,x0,e,pr)\n",
    "print(\"Number of iterations = \", it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the number of iteration it took for the system to converge using the initial guesses of **x0 = [0,0,0,0]** and **x0 = [1,1,1,1]**, we can observe that they both took 6 iterations to converge with consistency of 4 decimal places. If we analyze the error in each iteration, we can observe that using **x0 = [1,1,1,1]** helped reduce the error faster. Using **x0 = [0,0,0,0]** we have a final error of 4.14e-5 compared to 2.29e-5 using **x0 = [1,1,1,1]**.\n",
    "\n",
    "We will try using a consistency of 5, 6 and 7 decimal places to see if using **x0 = [1,1,1,1]** makes the method converge with lower iterations than using **x0 = [0,0,0,0]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6         2.32727273 -0.98727273  0.87886364]\n",
      "-----------------\n",
      "error =  0.18484375000000003\n",
      "[ 1.03018182  2.03693802 -1.0144562   0.98434122]\n",
      "-----------------\n",
      "error =  0.016388814658793216\n",
      "[ 1.00658504  2.00355502 -1.00252738  0.99835095]\n",
      "-----------------\n",
      "error =  0.002856953090344185\n",
      "[ 1.00086098  2.00029825 -1.00030728  0.99984975]\n",
      "-----------------\n",
      "error =  0.0003847917873479008\n",
      "[ 1.00009128  2.00002134 -1.00003115  0.9999881 ]\n",
      "-----------------\n",
      "error =  4.1457869928006095e-05\n",
      "[ 1.00000836  2.00000117 -1.00000275  0.99999922]\n",
      "-----------------\n",
      "error =  3.848654328653028e-06\n",
      "[ 1.00000067  2.00000002 -1.00000021  0.99999996]\n",
      "-----------------\n",
      "error =  3.110314673072606e-07\n",
      "[ 1.00000004  1.99999999 -1.00000001  1.        ]\n",
      "-----------------\n",
      "error =  2.1116799365537246e-08\n",
      "[ 1.  2. -1.  1.]\n",
      "-----------------\n",
      "Number of iterations =  9\n"
     ]
    }
   ],
   "source": [
    "e = 0.0000001\n",
    "x0 = np.zeros(len(A),float)\n",
    "pr = True\n",
    "x, it = GS(A,x0,e,pr)\n",
    "print(\"Number of iterations = \", it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5         2.13636364 -0.88636364  0.96306818]\n",
      "-----------------\n",
      "error =  0.22978723404255322\n",
      "[ 0.99090909  2.01957645 -0.99991736  0.99266916]\n",
      "-----------------\n",
      "error =  0.008612275598771455\n",
      "[ 1.00194112  2.0021833  -1.00090298  0.99906839]\n",
      "-----------------\n",
      "error =  0.0009864457246363382\n",
      "[ 1.00039893  2.00020825 -1.00015212  0.99990289]\n",
      "-----------------\n",
      "error =  0.0001738197994263564\n",
      "[ 1.00005125  2.00001731 -1.00001823  0.99999123]\n",
      "-----------------\n",
      "error =  2.293582118326794e-05\n",
      "[ 1.00000538  2.00000122 -1.00000183  0.99999931]\n",
      "-----------------\n",
      "error =  2.444438770033308e-06\n",
      "[ 1.00000049  2.00000007 -1.00000016  0.99999996]\n",
      "-----------------\n",
      "error =  2.249473141715138e-07\n",
      "[ 1.00000004  2.         -1.00000001  1.        ]\n",
      "-----------------\n",
      "error =  1.8007612280072827e-08\n",
      "[ 1.  2. -1.  1.]\n",
      "-----------------\n",
      "Number of iterations =  9\n"
     ]
    }
   ],
   "source": [
    "e = 0.0000001\n",
    "x0 = np.ones(len(A),float)\n",
    "pr = True\n",
    "x, it = GS(A,x0,e, pr)\n",
    "print(\"Number of iterations = \", it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using lower errors to test the convergence of the **Gauss-Seidel method** we observe that even though using **x0 = [1,1,1,1]** produces slightly lower error than using **x0 = [0,0,0,0]**, the number of iterations for convergence up to 4,5,6 and 7 decimal places does not change. Above we show the example with consistency up to 7 decimal places and we see that in both cases it took 9 iteration for the method to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOR Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use our **SOR** function described above to solve the **Ax = b**. We will use an acceleration factor **w = 1.1** to test if the system converges faster than the regular Gauss-Seidel method. Again, we will iterate until the method reaches consistency to the tolerance $\\varepsilon$ using  the **infinity norm** as convergence criteria.We will use **x0 = [0,0,0,0]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66        2.566      -1.07294     0.85649575]\n",
      "-----------------\n",
      "error =  0.22432269875292288\n",
      "[ 1.1123068   1.99038796 -1.03425629  1.01360515]\n",
      "-----------------\n",
      "error =  0.05881186187694755\n",
      "[ 0.99524838  1.99297887 -0.99480477  1.00225005]\n",
      "-----------------\n",
      "error =  0.0037249485598974\n",
      "[ 0.99855989  2.00040261 -0.99991091  0.99962117]\n",
      "-----------------\n",
      "error =  0.0008042432684686864\n",
      "[ 1.0001687   2.00009917 -1.00007679  0.99998642]\n",
      "-----------------\n",
      "error =  7.887918876190647e-05\n",
      "[ 1.00001093  1.99998757 -0.99999759  1.00000682]\n",
      "-----------------\n",
      "Number of iterations =  6\n"
     ]
    }
   ],
   "source": [
    "e = 0.0001\n",
    "w = 1.1\n",
    "g = np.zeros(len(A),float) \n",
    "pr = True\n",
    "x, it = SOR(A,g,w,e,pr)\n",
    "print(\"Number of iterations = \", it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can observe that the number of iterations to reach convergence (6) were the same as when using the regular Gauss-Seidel method. Also we can noticed that the final error was higher with 7.88e-5 compared to 4.14e-5 when using the regular Gauss-Seidel method with **x0 = [0,0,0,0]**. This means that in this case, introducing the acceleration factor is actually harming the iterative method by incrementing the error. We actually tested different acceleration factors between 1 and 2 but the results where even worse resulting in more iterations (See example below with **w = 1.3**) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78        3.04672727 -1.23672545  0.75125257]\n",
      "-----------------\n",
      "error =  0.4166353760843501\n",
      "[ 1.26362316  1.77735291 -1.05880567  1.17360876]\n",
      "-----------------\n",
      "error =  0.20050309496097163\n",
      "[ 0.9072584   1.9873318  -0.93732321  0.9642781 ]\n",
      "-----------------\n",
      "error =  0.051637699664571\n",
      "[ 1.00987965  2.02504035 -1.02276035  0.99481084]\n",
      "-----------------\n",
      "error =  0.01613238325660599\n",
      "[ 1.00620904  1.99237162 -0.99645253  1.00585205]\n",
      "-----------------\n",
      "error =  0.005012009929900946\n",
      "[ 0.99622326  2.0001866  -0.99929726  0.99826761]\n",
      "-----------------\n",
      "error =  0.002375435885586547\n",
      "[ 1.00097457  2.00075646 -1.00059108  1.00005489]\n",
      "-----------------\n",
      "error =  0.0005385359943596845\n",
      "[ 0.99995965  1.99967898 -0.99984678  1.00016493]\n",
      "-----------------\n",
      "error =  0.0001844061145055497\n",
      "[ 0.99993054  2.00004773 -1.00000026  0.99992721]\n",
      "-----------------\n",
      "error =  4.8286884791538394e-05\n",
      "[ 1.00002711  2.00001466 -1.00001453  1.00001233]\n",
      "-----------------\n",
      "Number of iterations =  10\n"
     ]
    }
   ],
   "source": [
    "e = 0.0001\n",
    "w = 1.3\n",
    "g = np.zeros(len(A),float)\n",
    "pr = True\n",
    "x, it = SOR(A,g,w,e,pr)\n",
    "print(\"Number of iterations = \", it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude our report, we will loop over 10 different tolerances $\\varepsilon$, ranging from 1e-2 to 1e-11 and **w = 1.1**. In each step we will apply the **Gauss-Seidel method** using **x0 = [0,0,0,0]** and **x0 = [1,1,1,1]**, and SOR using **x0 = [0,0,0,0]**. We will report how many iterations it took each method to converge for each $\\varepsilon$ and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((10,4))\n",
    "w = 1.1\n",
    "e = 0.01\n",
    "pr = False\n",
    "for i in range(10):\n",
    "    x0_1 = np.zeros(len(A),float)\n",
    "    x0_2 = np.ones(len(A),float)\n",
    "    x0_3 = np.zeros(len(A),float)\n",
    "    \n",
    "    results[i][0] = e\n",
    "    \n",
    "    x, it_1 = GS(A,x0_1,e,pr)\n",
    "    results[i][1] = it_1\n",
    "    \n",
    "    x, it_2 = GS(A,x0_2,e,pr)\n",
    "    results[i][2] = it_2\n",
    "    \n",
    "    x, it_3 = SOR(A,x0_3,w,e,pr)\n",
    "    results[i][3] = it_3\n",
    "    \n",
    "    e = e/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations to reach Convergence\n",
      "              G-S x0 = zeros|  G-S x0 = ones|  SOR x0 = zeros\n",
      "epsilon                                                      \n",
      "1.000000e-02                4               3               4\n",
      "1.000000e-03                5               4               5\n",
      "1.000000e-04                6               6               6\n",
      "1.000000e-05                7               7               7\n",
      "1.000000e-06                8               8               9\n",
      "1.000000e-07                9               9              10\n",
      "1.000000e-08               10              10              11\n",
      "1.000000e-09               11              11              12\n",
      "1.000000e-10               11              11              13\n",
      "1.000000e-11               12              12              14\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(results,columns =[\"epsilon\", \"G-S x0 = zeros|\", \"G-S x0 = ones|\", \"SOR x0 = zeros\"])\n",
    "df = df.set_index('epsilon')\n",
    "df[\"G-S x0 = zeros|\"] = df[\"G-S x0 = zeros|\"].astype(int)\n",
    "df[\"G-S x0 = ones|\"] = df[\"G-S x0 = ones|\"].astype(int)\n",
    "df[\"SOR x0 = zeros\"] = df[\"SOR x0 = zeros\"].astype(int)\n",
    "print(\"Number of Iterations to reach Convergence\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above we can observe that the best method was the **Gauss-Seidel** intialized in **x0 = [1,1,1,1]**. In every case it had equal or lower iterations than the **Gauss-Seidel** intialized in **x0 = [0,0,0,0]** and the **SOR** method. It was surprising that the **SOR** method performed the worst since the whole purpose of introducing an over-relaxed acceleration factor was to accelerate convergence. Further analysis has to be made in order to evaluate why this acceleration factor of **w= 1.1** is not helping the system to converge faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
